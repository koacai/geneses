optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-4
lr_scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau

vocab_size: 2048
scheduler_n: 2.0

flow_predictor:
  mimi_embedding:
    vocab_size: ${model.vocab_size}
    hidden_size: 512
    num_codebooks: ${data.preprocess.mimi.num_codebooks}
  logits_head:
    vocab_size: ${model.vocab_size}
    hidden_size: ${model.flow_predictor.mimi_embedding.hidden_size}
    num_codebooks: ${data.preprocess.mimi.num_codebooks}
  decoder:
    hidden_size: ${model.flow_predictor.mimi_embedding.hidden_size}
    channels: [256, 256]
    dropout: 0.05
    attention_head_dim: 64
    n_blocks: 1
    num_mid_blocks: 2
    num_heads: 2
    act_fn: snakebeta

mimi: ${data.preprocess.mimi}
