optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-4
lr_scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau

vocab_size: 2048
scheduler_n: 2.0

flow_predictor:
  vocab_size: ${model.vocab_size}
  hidden_size: 512
  num_codebooks: ${data.preprocess.mimi.num_codebooks}
  channels: [256, 256]
  dropout: 0.05
  attention_head_dim: 64
  n_blocks: 1
  num_mid_blocks: 2
  num_heads: 2
  act_fn: snakebeta

mimi: ${data.preprocess.mimi}
