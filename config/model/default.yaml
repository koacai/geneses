optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-4

vocab_size: 500
scheduler_n: 2.0

flow_predictor:
  vocab_size: ${model.vocab_size}
  hidden_size: 512
  channels: [256, 256]
  dropout: 0.05
  attention_head_dim: 64
  n_blocks: 1
  num_mid_blocks: 2
  num_heads: 2
  act_fn: snakebeta

hifigan:
  repo: koacai/hifigan
  filename: hubert_base_token/HQYouTube/epoch=14-step=1118540.ckpt
